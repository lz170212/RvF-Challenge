{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RvF: Real vs Fake face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To understand CNN theoretically, other than using online resources, you can refer to [CNN Crash Course (U-M Only)](https://docs.google.com/presentation/d/1p3EWFMfTNT773PEt3q16tlLxQ4FuD-JTwnTj1A_N4a0/edit?usp=sharing)\n",
    "\n",
    "Here is also a guide on [PyTorch for CNNS](https://github.com/MichiganDataScienceTeam/W24-RvF/blob/main/notebooks/pytorch_cnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from starter_code.dataset import RvFDataset, get_loaders\n",
    "from starter_code.train import train_model, plot_performance, load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Preprocessing\n",
    "\n",
    "There are numerous advantages for preprocessing, depending on the topic of your program. For our project on CNN for RvF, the two major benefits of preprocessing are:\n",
    "\n",
    "### Normalization\n",
    "Normalization brings features onto a similar scale, preventing certain features from dominating the learning process due to larger magnitude. By normalizing the data, we ensure that each feature contributes proportionally to the learning process, leading to efficient convergence and model generalization.\n",
    "\n",
    "### Generalization\n",
    "Preprocessing techniques help to generalize the model better to unseen data by introducing variability in the training images. This prevents the model from overfitting to the training data. Examples of such preprocessing are random crop, random jitter, etc.\n",
    "\n",
    "Here is a more detailed guide on [Image Preprocessing](https://github.com/MichiganDataScienceTeam/W24-RvF/blob/main/notebooks/image_preprocessing.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "train_dataset = RvFDataset(\"train\", data_directory = \"data/rvf10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# normalization is implemented for you, which is called in the next cell\n",
    "mean = torch.zeros((3,))\n",
    "variance = torch.zeros((3,))\n",
    "tensor_converter = v2.ToTensor()\n",
    "\n",
    "for image, _ in train_dataset:\n",
    "    mean += tensor_converter(image).mean(dim=(1, 2))\n",
    "    mean /= len(train_dataset)\n",
    "for image, _ in train_dataset:\n",
    "    image = tensor_converter(image)\n",
    "    variance += ((image - mean.view(3, 1, 1))**2).mean(dim=(1, 2))\n",
    "\n",
    "std = torch.sqrt(variance / len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO1: Define Your Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image) -> torch.Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocesses an image by applying a series of transformation.\n",
    "\n",
    "    Args:\n",
    "        image (npt.ArrayLike): The input image to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The preprocessed image as a tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    tensor = torch.tensor(image, dtype = torch.float32).permute(2, 0, 1) # convert image to tensor\n",
    "\n",
    "    tensor = v2.Normalize(mean = mean, std = std)(tensor)\n",
    "\n",
    "    # TODO: Add more preprocessing steps to improve model performance.\n",
    "    \n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Definition\n",
    "\n",
    "Below is an example of a class definition in Python for a very simple convolutional neural network called BasicCNN. Let's break its components down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module): # Net inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor for the neural network.\"\"\"\n",
    "        super(BasicCNN, self).__init__()        # Call superclass constructor\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=128, kernel_size=3, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()              \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(3200, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = self.conv1(x)\n",
    "        h1 = self.relu(z1)\n",
    "        p1 = self.pool(h1)\n",
    "\n",
    "        z2 = self.conv2(p1)\n",
    "        h2 = self.relu(z2)\n",
    "        p2 = self.pool(h2)\n",
    "\n",
    "        flat = self.flatten(p2)\n",
    "        z = self.fc(flat)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subclass Inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first criteria is met by defining the subclass relationship between `BasicCNN` and `nn.Module`\n",
    "- When we write the first line of the class defintion, we write `BasicCNN(nn.Module):` to indicate that `BasicCNN` is a subclass of `nn.Module`\n",
    "- On line 4, we call the superclass constructor for this model:\n",
    "  \n",
    "  ```py\n",
    "    super(BasicCNN, self).__init__() \n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PyTorch to recognize that a layer is a part of our model, we must add all them as **member variables** of the `BasicCNN`. This can be done in the class constructor `__init__()` by evoking the `self` pointer:\n",
    "\n",
    "```py\n",
    "self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1)\n",
    "self.conv2 = nn.Conv2d(in_channels=16, out_channels=128, kernel_size=3, stride=1)\n",
    "self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "self.relu = nn.ReLU()              \n",
    "self.flatten = nn.Flatten()\n",
    "self.fc = nn.Linear(3200, 10) \n",
    "```\n",
    "\n",
    "This code defines 5 layers for our model:\n",
    "- `conv1`: convolution layer that expects 1 channel and has 16 filters with filter size of 3 pixels and a stride of 1\n",
    "- `conv2`: convolution layer that expects 16 channel and has 128 filters with filter size of 3 pixels and a stride of 1\n",
    "- `pool`: max pooling layer that has a window size of 2 and a stride of 2. We will reuse this layer multiple times (since max pooling is stateless)\n",
    "- `relu`: activation layer using the ReLU activation function. We will reuse this activation layer multiple times (since activation functions are stateless)\n",
    "- `fc`: a dense layer that expects a vector with 3200 components and returns a vector with 10 components (one for each of the 10 classes in the MNIST dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third criteria is more tricky - we have to define a function called `forward()` that specifies _how_ to call each layer and make predictions for some input image. For the model above, we have the following definition for this function\n",
    "\n",
    "```py\n",
    "def forward(self, x):\n",
    "   z1 = self.conv1(x)\n",
    "   h1 = self.relu(z1)\n",
    "   p1 = self.pool(h1)\n",
    "\n",
    "   z2 = self.conv2(p1)\n",
    "   h2 = self.relu(z2)\n",
    "   p2 = self.pool(h2)\n",
    "\n",
    "   flat = self.flatten(p2)\n",
    "   z = self.fc(flat)\n",
    "\n",
    "   return z\n",
    "```\n",
    "\n",
    "Let's break down the first few lines of this function:\n",
    "1. The `forward()` function takes as input the parameters\n",
    "   1. `self` - is the self-pointer, is equivalent to `this` in C++\n",
    "   2. `x` - the input to the model - in this case an image of a handwritten digit.\n",
    "2. The image `x` is immediately passed as input into the first convolution layer `conv1` to perform convolution. The output of this convolution layer is saved to the local variable `z1`.\n",
    "   1. Note that in this case, `self.conv` is actually a **functor** - it is an object that can be called like a function to produce an output\n",
    "3. The convolution layer output `z1` is passed through the ReLU activation layer to get the activated outputs `h1`\n",
    "4. The activated output has max pooling applied to downsample it, and the output is then saved the result to `p1`.\n",
    "5. The pooled output is passed as input the second convolution layer `conv2` to perform another round of convolution. The output of this convolution layer is saved to the local variable `z2`\n",
    "   1. 🚨 As `p1` has 16 channels, we MUST define `conv2` to accept 16 input channels. It is SUPER important to be careful to make sure that your input to your convolution layer has the correct number of channels, otherwise PyTorch will throw errors!\n",
    "6. ...\n",
    "\n",
    "And so forth! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO2: Define Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the restrictions:\n",
    "- your first convolution layer must accept images that have only 4 channels\n",
    "- your last convolution layer must accept images that have only 256 channels\n",
    "- your CNN output should return a vector with 2 entries\n",
    "- if you find difficulties in understanding the model, visit [Pytorch and CNN](https://github.com/MichiganDataScienceTeam/W24-RvF/blob/main/notebooks/pytorch_cnn.ipynb)\n",
    "\n",
    "Other than that, you have as much flexibility as you prefer for how you want to define your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      \"\"\"Constructor for the neural network.\"\"\"\n",
    "      super(Model, self).__init__()        # Call superclass constructor\n",
    "\n",
    "      # A few preprocessing ... \n",
    "      # You are free to add more or delete \n",
    "      self.batchnorm = torch.nn.BatchNorm2d(num_features = 3)\n",
    "      self.padding = torch.nn.ZeroPad2d(padding = 2)\n",
    "      self.dropout = torch.nn.Dropout(p = 0.50)\n",
    "\n",
    "      # TODO: define your convolution layer, max pooling layer, activation layer, and dense layer\n",
    "\n",
    "    # TODO: define your forward function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model training, we utilized functions wirtten in starter_code directory located under \"Optional-Challenge/RvF\". They are ready to use, and feel free to refer back to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_loaders(batch_size = 16, preprocessor = preprocess, data_directory = \"data/rvf10k\")\n",
    "\n",
    "model = Model()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4) # TODO: Change the optimizer to explore different options\n",
    "criterion = torch.nn.CrossEntropyLoss() # TODO: Change the criterion to explore different options\n",
    "\n",
    "history = train_model(model, criterion, optimizer, train_loader, val_loader)\n",
    "plot_performance(history)\n",
    "\n",
    "# Load the model from the training run\n",
    "load_model(model, \"checkpoints\", 10) # you can modify the number of epochs, currently set at 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of RvF challenge! Please save your file and submit your work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
